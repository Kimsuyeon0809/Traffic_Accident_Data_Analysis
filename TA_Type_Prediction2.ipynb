{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPn2nqFv6oq3"
   },
   "source": [
    "------------------------------\n",
    "1. 입력 피처 (Features)\n",
    "모델의 입력 피처로 사용된 변수는 아래와 같습니다:\n",
    "\n",
    "- hour: 사고 발생 시간.\n",
    "\n",
    "- is_holiday: 공휴일 여부.\n",
    "\n",
    "- road_form_class: 도로 형태를 나타내는 변수.\n",
    "\n",
    "- road_formD: 상세 도로 형태를 나타내는 변수.\n",
    "\n",
    "- carFLg: 사고 차량의 플래그.\n",
    "\n",
    "- carClassF: 사고 차량의 분류.\n",
    "\n",
    "- carClassVic: 피해 차량의 분류.\n",
    "\n",
    "- lo_crd: 사고 위치의 경도 (longitudinal coordinate).\n",
    "\n",
    "- la_crd: 사고 위치의 위도 (latitudinal coordinate).\n",
    "\n",
    "- grid_id: 사고 위치를 2km x 2km 간격으로 격자화하여 생성한 범주형 변수.\n",
    "\n",
    " - 이 피처들은 모델의 입력 데이터(X_combined)로 사용됩니다.\n",
    "\n",
    "-------------------------\n",
    "\n",
    " 2. 예측 대상 (Target)\n",
    "\n",
    " - 모델의 예측 대상(종속 변수)은 **accTypeD_merged_combined**입니다.\n",
    "\n",
    "이는 원래의 사고 유형(accTypeD)에서 일부 클래스를 통합한 변수입니다.\n",
    "최종적으로 아래와 같은 클래스가 예측 대상입니다\n",
    "\n",
    "  1. 0 : 횡단중\n",
    "  2. 1 : 차도통행중\n",
    "  3. 2 (통합된 클래스)\n",
    "    - 원래 클래스(2, 6, 7, 8)\n",
    "      - 2 : 길가장자리구역통행중\n",
    "      - 6 : 도로이탈\n",
    "      - 7 : 전도전복\n",
    "      - 8 : 기타\n",
    "  4. 3 : 추돌 (뒤에서 박은거)\n",
    "  5. 4 : 충돌 (뒤 말고 다르게 다 박은거)\n",
    "\n",
    "  --------------------\n",
    "  3. 삭제된 클래스\n",
    "  - 5번, 9번 클래스 샘플 수 너무 적어서 삭제\n",
    "    - 5 : 보도통행중\n",
    "    - 9 : 기타"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Na8O9TWFb4c"
   },
   "source": [
    "-------------------------------\n",
    "SOMTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21748,
     "status": "ok",
     "timestamp": 1732517460144,
     "user": {
      "displayName": "이은화",
      "userId": "10845709387540316486"
     },
     "user_tz": -540
    },
    "id": "elVFBNl_Fdya",
    "outputId": "939e0191-9527-474c-c365-04410eb3cde3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "<ipython-input-1-0a39743bc996>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
      "<ipython-input-1-0a39743bc996>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02  # 2km 격자화\n",
      "<ipython-input-1-0a39743bc996>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
      "<ipython-input-1-0a39743bc996>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_id'] = data_filtered['grid_lat'].astype(str) + '_' + data_filtered['grid_lon'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 823\n",
      "[LightGBM] [Info] Number of data points in the train set: 3315, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "LightGBM Accuracy after SMOTE: 0.6202247191011236\n",
      "LightGBM Classification Report after SMOTE:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.70      0.65        76\n",
      "           1       0.44      0.33      0.38        21\n",
      "           2       0.53      0.52      0.52       139\n",
      "           3       0.64      0.75      0.69        48\n",
      "           4       0.72      0.67      0.70       161\n",
      "\n",
      "    accuracy                           0.62       445\n",
      "   macro avg       0.59      0.59      0.59       445\n",
      "weighted avg       0.62      0.62      0.62       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 1. 데이터 로드\n",
    "data = pd.read_csv('TA_cleaned.csv')  # 실제 파일 경로로 수정\n",
    "\n",
    "# 2. 클래스 9 및 클래스 5 삭제\n",
    "data_filtered = data[data['accTypeD'].isin([0, 1, 2, 3, 4, 6, 7, 8])]\n",
    "\n",
    "# 3. 클래스 2, 6, 7, 8 통합\n",
    "data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
    "\n",
    "# 4. 격자화 및 피처 정의\n",
    "features_with_grid = ['hour', 'is_holiday', 'road_form_class', 'road_formD', 'carFLg',\n",
    "                      'carClassF', 'carClassVic', 'lo_crd', 'la_crd']\n",
    "data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02  # 2km 격자화\n",
    "data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
    "data_filtered['grid_id'] = data_filtered['grid_lat'].astype(str) + '_' + data_filtered['grid_lon'].astype(str)\n",
    "\n",
    "# 범주형 변수 목록 정의\n",
    "categorical_features = ['is_holiday', 'road_form_class', 'road_formD', 'carFLg',\n",
    "                        'carClassF', 'carClassVic', 'grid_id']\n",
    "\n",
    "# 5. 범주형 변수 라벨 인코딩\n",
    "data_encoded = data_filtered.copy()\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data_encoded[col] = le.fit_transform(data_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 6. 특성과 레이블 정의\n",
    "X = data_encoded[features_with_grid + ['grid_id']]\n",
    "y = data_encoded['accTypeD_merged_combined']\n",
    "\n",
    "# 7. 데이터 분리\n",
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 범주형 변수의 인덱스 식별\n",
    "categorical_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "# 8. SMOTENC를 사용하여 데이터 증강\n",
    "smotenc = SMOTENC(categorical_features=categorical_indices, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotenc.fit_resample(X_train_combined, y_train_combined)\n",
    "\n",
    "# 9. LightGBM 모델 학습\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "lgbm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 10. 예측 및 성능 평가\n",
    "y_pred_lgbm_resampled = lgbm_model.predict(X_test_combined)\n",
    "accuracy_lgbm_resampled = accuracy_score(y_test_combined, y_pred_lgbm_resampled)\n",
    "report_lgbm_resampled = classification_report(\n",
    "    y_test_combined, y_pred_lgbm_resampled, target_names=[str(cls) for cls in np.unique(y_train_combined)]\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"LightGBM Accuracy after SMOTE:\", accuracy_lgbm_resampled)\n",
    "print(\"LightGBM Classification Report after SMOTE:\\n\", report_lgbm_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z5utoAFJcjc"
   },
   "source": [
    "---------------------------------------\n",
    "앙상블~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9769,
     "status": "ok",
     "timestamp": 1732518520201,
     "user": {
      "displayName": "이은화",
      "userId": "10845709387540316486"
     },
     "user_tz": -540
    },
    "id": "OrKu3aAuJnVa",
    "outputId": "03950312-5a94-433c-f0e9-9bd84a451b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
      "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: catboost\n",
      "Successfully installed catboost-1.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18104,
     "status": "ok",
     "timestamp": 1732518540459,
     "user": {
      "displayName": "이은화",
      "userId": "10845709387540316486"
     },
     "user_tz": -540
    },
    "id": "69L-HGDXJfDP",
    "outputId": "79178147-6a30-4bf2-9580-fbc48df4d2af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-de231adb9d05>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
      "<ipython-input-10-de231adb9d05>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02  # 2km 격자화\n",
      "<ipython-input-10-de231adb9d05>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
      "<ipython-input-10-de231adb9d05>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_id'] = data_filtered['grid_lat'].astype(str) + '_' + data_filtered['grid_lon'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3725, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:08:44] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy after SMOTE: 0.6681614349775785\n",
      "Ensemble Model Classification Report after SMOTE:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67        41\n",
      "           1       0.10      0.12      0.11         8\n",
      "           2       0.58      0.59      0.58        63\n",
      "           3       0.71      0.78      0.75        32\n",
      "           4       0.80      0.75      0.77        79\n",
      "\n",
      "    accuracy                           0.67       223\n",
      "   macro avg       0.57      0.58      0.58       223\n",
      "weighted avg       0.68      0.67      0.67       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 1. 데이터 로드\n",
    "data = pd.read_csv('TA_cleaned.csv')  # 실제 파일 경로로 수정\n",
    "\n",
    "# 2. 클래스 9 및 클래스 5 삭제\n",
    "data_filtered = data[data['accTypeD'].isin([0, 1, 2, 3, 4, 6, 7, 8])]\n",
    "\n",
    "# 3. 클래스 2, 6, 7, 8 통합\n",
    "data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
    "\n",
    "# 4. 격자화 및 피처 정의\n",
    "features_with_grid = ['hour', 'is_holiday', 'road_form_class', 'road_formD', 'carFLg',\n",
    "                      'carClassF', 'carClassVic', 'lo_crd', 'la_crd']\n",
    "data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02  # 2km 격자화\n",
    "data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
    "data_filtered['grid_id'] = data_filtered['grid_lat'].astype(str) + '_' + data_filtered['grid_lon'].astype(str)\n",
    "\n",
    "# 범주형 변수 목록 정의\n",
    "categorical_features = ['is_holiday', 'road_form_class', 'road_formD', 'carFLg',\n",
    "                        'carClassF', 'carClassVic', 'grid_id']\n",
    "\n",
    "# 5. 범주형 변수 라벨 인코딩\n",
    "data_encoded = data_filtered.copy()\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data_encoded[col] = le.fit_transform(data_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 6. 특성과 레이블 정의\n",
    "X = data_encoded[features_with_grid + ['grid_id']]\n",
    "y = data_encoded['accTypeD_merged_combined']\n",
    "\n",
    "# 7. 데이터 분리\n",
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# 범주형 변수의 인덱스 식별\n",
    "categorical_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "# 8. SMOTENC를 사용하여 데이터 증강\n",
    "smotenc = SMOTENC(categorical_features=categorical_indices, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotenc.fit_resample(X_train_combined, y_train_combined)\n",
    "\n",
    "# 9. 모델 정의\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "catboost_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# 10. 앙상블 모델 생성 (VotingClassifier)\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lightgbm', lgbm_model),\n",
    "        ('xgboost', xgb_model),\n",
    "        ('catboost', catboost_model)\n",
    "    ],\n",
    "    voting='soft'  # 확률값을 사용한 소프트 보팅\n",
    ")\n",
    "\n",
    "# 앙상블 모델 학습\n",
    "ensemble_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 11. 예측 및 성능 평가\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_combined)\n",
    "accuracy_ensemble = accuracy_score(y_test_combined, y_pred_ensemble)\n",
    "report_ensemble = classification_report(\n",
    "    y_test_combined, y_pred_ensemble, target_names=[str(cls) for cls in np.unique(y_train_combined)]\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Ensemble Model Accuracy after SMOTE:\", accuracy_ensemble)\n",
    "print(\"Ensemble Model Classification Report after SMOTE:\\n\", report_ensemble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rzb23S8LlqO"
   },
   "source": [
    "------------------------------------------------\n",
    "\n",
    "**클래스 1 데이터가 부족하여 보완**\n",
    "\n",
    "\n",
    "1. 데이터 증강: SMOTENC 또는 ADASYN으로 클래스 1 샘플 증가.\n",
    "2. 클래스 가중치: 소수 클래스에 가중치를 부여.\n",
    "3. Threshold 조정: 클래스 1에 더 낮은 임계값 설정.\n",
    "4. Focal Loss: 소수 클래스 예측에 민감한 손실 함수 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6660,
     "status": "ok",
     "timestamp": 1732518935332,
     "user": {
      "displayName": "이은화",
      "userId": "10845709387540316486"
     },
     "user_tz": -540
    },
    "id": "a34rJ_T_K-Op",
    "outputId": "13847b2b-155a-4280-a45d-eda0afd4651a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-87377b5bf284>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
      "<ipython-input-12-87377b5bf284>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02  # 2km 격자화\n",
      "<ipython-input-12-87377b5bf284>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
      "<ipython-input-12-87377b5bf284>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_id'] = data_filtered['grid_lat'].astype(str) + '_' + data_filtered['grid_lon'].astype(str)\n",
      "<ipython-input-12-87377b5bf284>:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['is_night'] = data_filtered['hour'].apply(lambda x: 1 if x < 6 or x >= 18 else 0)\n",
      "<ipython-input-12-87377b5bf284>:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['is_rush_hour'] = data_filtered['hour'].apply(lambda x: 1 if 7 <= x <= 9 or 17 <= x <= 19 else 0)\n",
      "<ipython-input-12-87377b5bf284>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['holiday_road_combo'] = data_filtered['is_holiday'].astype(str) + '_' + data_filtered['road_form_class'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 1852, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -2.039211\n",
      "[LightGBM] [Info] Start training from score -1.205068\n",
      "[LightGBM] [Info] Start training from score -1.452476\n",
      "[LightGBM] [Info] Start training from score -2.748257\n",
      "[LightGBM] [Info] Start training from score -1.301338\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:15:29] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy after ADASYN and Class Weighting: 0.6202247191011236\n",
      "Ensemble Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.67      0.63        76\n",
      "           1       0.22      0.19      0.21        21\n",
      "           2       0.56      0.53      0.55       139\n",
      "           3       0.70      0.67      0.68        48\n",
      "           4       0.70      0.71      0.71       161\n",
      "\n",
      "    accuracy                           0.62       445\n",
      "   macro avg       0.56      0.55      0.55       445\n",
      "weighted avg       0.62      0.62      0.62       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTENC, ADASYN\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# 1. 데이터 로드\n",
    "data = pd.read_csv('TA_cleaned.csv')  # 실제 파일 경로로 수정\n",
    "\n",
    "# 2. 클래스 9 및 클래스 5 삭제\n",
    "data_filtered = data[data['accTypeD'].isin([0, 1, 2, 3, 4, 6, 7, 8])]\n",
    "\n",
    "# 3. 클래스 2, 6, 7, 8 통합\n",
    "data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
    "\n",
    "# 4. 격자화 및 피처 정의\n",
    "features_with_grid = ['hour', 'is_holiday', 'road_form_class', 'road_formD', 'carFLg',\n",
    "                      'carClassF', 'carClassVic', 'lo_crd', 'la_crd']\n",
    "data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02  # 2km 격자화\n",
    "data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
    "data_filtered['grid_id'] = data_filtered['grid_lat'].astype(str) + '_' + data_filtered['grid_lon'].astype(str)\n",
    "\n",
    "# 특화된 피처 추가\n",
    "data_filtered['is_night'] = data_filtered['hour'].apply(lambda x: 1 if x < 6 or x >= 18 else 0)\n",
    "data_filtered['is_rush_hour'] = data_filtered['hour'].apply(lambda x: 1 if 7 <= x <= 9 or 17 <= x <= 19 else 0)\n",
    "data_filtered['holiday_road_combo'] = data_filtered['is_holiday'].astype(str) + '_' + data_filtered['road_form_class'].astype(str)\n",
    "\n",
    "# 범주형 변수 목록 정의\n",
    "categorical_features = ['is_holiday', 'road_form_class', 'road_formD', 'carFLg',\n",
    "                        'carClassF', 'carClassVic', 'grid_id', 'holiday_road_combo']\n",
    "\n",
    "# 5. 범주형 변수 라벨 인코딩\n",
    "data_encoded = data_filtered.copy()\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data_encoded[col] = le.fit_transform(data_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 6. 특성과 레이블 정의\n",
    "X = data_encoded[features_with_grid + ['grid_id', 'is_night', 'is_rush_hour', 'holiday_road_combo']]\n",
    "y = data_encoded['accTypeD_merged_combined']\n",
    "\n",
    "# 7. 데이터 분리\n",
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 범주형 변수의 인덱스 식별\n",
    "categorical_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "# 8. 데이터 증강 (ADASYN)\n",
    "# ADASYN으로 클래스 1 샘플 수 증강 (클래스 1: 150개 생성)\n",
    "adasyn = ADASYN(sampling_strategy={1: 150}, random_state=42)\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_combined, y_train_combined)\n",
    "\n",
    "# 9. 앙상블 모델 정의\n",
    "class_weights = {0: 1, 1: 5, 2: 1, 3: 1, 4: 1}\n",
    "\n",
    "lgbm_model = LGBMClassifier(random_state=42, class_weight=class_weights)\n",
    "xgb_model = XGBClassifier(random_state=42, scale_pos_weight=5, use_label_encoder=False, eval_metric='mlogloss')\n",
    "catboost_model = CatBoostClassifier(random_state=42, class_weights=class_weights, verbose=0)\n",
    "\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lightgbm', lgbm_model),\n",
    "        ('xgboost', xgb_model),\n",
    "        ('catboost', catboost_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# 10. 앙상블 모델 학습\n",
    "ensemble_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 11. Threshold 조정\n",
    "y_prob_ensemble = ensemble_model.predict_proba(X_test_combined)\n",
    "threshold = 0.3  # 클래스 1에 더 낮은 임계값 사용\n",
    "y_pred_adjusted = np.argmax(y_prob_ensemble, axis=1)\n",
    "\n",
    "# 12. 성능 평가\n",
    "accuracy_ensemble = accuracy_score(y_test_combined, y_pred_adjusted)\n",
    "report_ensemble = classification_report(y_test_combined, y_pred_adjusted, target_names=[str(cls) for cls in np.unique(y)])\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Ensemble Model Accuracy after ADASYN and Class Weighting:\", accuracy_ensemble)\n",
    "print(\"Ensemble Model Classification Report:\\n\", report_ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35462,
     "status": "ok",
     "timestamp": 1732519401291,
     "user": {
      "displayName": "이은화",
      "userId": "10845709387540316486"
     },
     "user_tz": -540
    },
    "id": "uldAKTYXM4VC",
    "outputId": "413cd01f-a4c4-406c-f5db-792fc74c6f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 1852, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -2.039211\n",
      "[LightGBM] [Info] Start training from score -1.205068\n",
      "[LightGBM] [Info] Start training from score -1.452476\n",
      "[LightGBM] [Info] Start training from score -2.748257\n",
      "[LightGBM] [Info] Start training from score -1.301338\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:22:47] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 837\n",
      "[LightGBM] [Info] Number of data points in the train set: 1481, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -2.041682\n",
      "[LightGBM] [Info] Start training from score -1.203460\n",
      "[LightGBM] [Info] Start training from score -1.452579\n",
      "[LightGBM] [Info] Start training from score -2.754790\n",
      "[LightGBM] [Info] Start training from score -1.300310\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 1481, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -2.039628\n",
      "[LightGBM] [Info] Start training from score -1.209989\n",
      "[LightGBM] [Info] Start training from score -1.450524\n",
      "[LightGBM] [Info] Start training from score -2.744704\n",
      "[LightGBM] [Info] Start training from score -1.298255\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 1482, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -2.038250\n",
      "[LightGBM] [Info] Start training from score -1.203973\n",
      "[LightGBM] [Info] Start training from score -1.453092\n",
      "[LightGBM] [Info] Start training from score -2.747271\n",
      "[LightGBM] [Info] Start training from score -1.302708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 1482, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -2.038250\n",
      "[LightGBM] [Info] Start training from score -1.203973\n",
      "[LightGBM] [Info] Start training from score -1.453092\n",
      "[LightGBM] [Info] Start training from score -2.747271\n",
      "[LightGBM] [Info] Start training from score -1.302708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 1482, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score -2.038250\n",
      "[LightGBM] [Info] Start training from score -1.203973\n",
      "[LightGBM] [Info] Start training from score -1.453092\n",
      "[LightGBM] [Info] Start training from score -2.747271\n",
      "[LightGBM] [Info] Start training from score -1.302708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:22:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:22:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:22:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:22:58] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:22:58] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Accuracy after ADASYN and Class Weighting: 0.6134831460674157\n",
      "Stacking Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65        76\n",
      "           1       0.27      0.14      0.19        21\n",
      "           2       0.56      0.47      0.51       139\n",
      "           3       0.72      0.58      0.64        48\n",
      "           4       0.66      0.75      0.70       161\n",
      "\n",
      "    accuracy                           0.61       445\n",
      "   macro avg       0.56      0.54      0.54       445\n",
      "weighted avg       0.60      0.61      0.60       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 기본 모델 정의\n",
    "lgbm_model = LGBMClassifier(random_state=42, class_weight=class_weights)\n",
    "xgb_model = XGBClassifier(random_state=42, scale_pos_weight=5, use_label_encoder=False, eval_metric='mlogloss')\n",
    "catboost_model = CatBoostClassifier(random_state=42, class_weights=class_weights, verbose=0)\n",
    "\n",
    "# StackingClassifier 정의\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lightgbm', lgbm_model),\n",
    "        ('xgboost', xgb_model),\n",
    "        ('catboost', catboost_model)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),  # 메타 모델로 로지스틱 회귀 사용\n",
    "    stack_method='predict_proba',  # 각 모델의 확률값을 스택으로 전달\n",
    "    cv=5  # 교차 검증\n",
    ")\n",
    "\n",
    "# Stacking 모델 학습\n",
    "stacking_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 성능 평가\n",
    "y_pred_stacking = stacking_model.predict(X_test_combined)\n",
    "accuracy_stacking = accuracy_score(y_test_combined, y_pred_stacking)\n",
    "report_stacking = classification_report(y_test_combined, y_pred_stacking, target_names=[str(cls) for cls in np.unique(y)])\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Stacking Model Accuracy after ADASYN and Class Weighting:\", accuracy_stacking)\n",
    "print(\"Stacking Model Classification Report:\\n\", report_stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5066,
     "status": "ok",
     "timestamp": 1732520664028,
     "user": {
      "displayName": "이은화",
      "userId": "10845709387540316486"
     },
     "user_tz": -540
    },
    "id": "-tIALPAiRzKb",
    "outputId": "eab15774-0dd7-4918-e858-7ab714615c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75002,
     "status": "ok",
     "timestamp": 1732520749110,
     "user": {
      "displayName": "이은화",
      "userId": "10845709387540316486"
     },
     "user_tz": -540
    },
    "id": "Z6xzBI4xNkmg",
    "outputId": "e7ae6635-1727-47be-cc0a-d16d0f1f094b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-a1e7caa116b8>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
      "<ipython-input-27-a1e7caa116b8>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02\n",
      "<ipython-input-27-a1e7caa116b8>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
      "<ipython-input-27-a1e7caa116b8>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['is_night'] = data_filtered['hour'].apply(lambda x: 1 if x < 6 or x >= 18 else 0)\n",
      "<ipython-input-27-a1e7caa116b8>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['holiday_road_combo'] = data_filtered['is_holiday'].astype(str) + '_' + data_filtered['road_form_class'].astype(str)\n",
      "[I 2024-11-25 07:44:34,927] A new study created in memory with name: no-name-3ee7e80c-a89b-4dff-8091-4991f65ee757\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:34] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:37,705] Trial 0 finished with value: 0.6224719101123596 and parameters: {'n_estimators': 177, 'learning_rate': 0.09498419629192881, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.5406887488442176, 'colsample_bytree': 0.8594825623189941, 'scale_pos_weight': 8.494777128402262}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:37] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:48,578] Trial 1 finished with value: 0.6134831460674157 and parameters: {'n_estimators': 317, 'learning_rate': 0.0332839405143591, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8026178250007346, 'colsample_bytree': 0.7614887130351375, 'scale_pos_weight': 9.75624558659241}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:48] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:48,971] Trial 2 finished with value: 0.6179775280898876 and parameters: {'n_estimators': 211, 'learning_rate': 0.17135756288256185, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.8388860481392857, 'colsample_bytree': 0.7901296196669975, 'scale_pos_weight': 6.412863668859987}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:48] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:49,216] Trial 3 finished with value: 0.6112359550561798 and parameters: {'n_estimators': 107, 'learning_rate': 0.05259092914540134, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.7297820525452869, 'colsample_bytree': 0.8699375544462036, 'scale_pos_weight': 6.636024113718971}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:49] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:50,308] Trial 4 finished with value: 0.5932584269662922 and parameters: {'n_estimators': 407, 'learning_rate': 0.11945414196911507, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.667872722367657, 'colsample_bytree': 0.5478290405598565, 'scale_pos_weight': 7.138934573155548}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:50] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:50,938] Trial 5 finished with value: 0.5955056179775281 and parameters: {'n_estimators': 299, 'learning_rate': 0.27835752838153577, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7591952019860617, 'colsample_bytree': 0.7247447053569307, 'scale_pos_weight': 6.868809250550166}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:50] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:52,363] Trial 6 finished with value: 0.5887640449438202 and parameters: {'n_estimators': 485, 'learning_rate': 0.23968449464029026, 'max_depth': 12, 'min_child_weight': 6, 'subsample': 0.959788294127729, 'colsample_bytree': 0.8757575110043453, 'scale_pos_weight': 4.5724884698039645}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:52] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:54,686] Trial 7 finished with value: 0.5910112359550562 and parameters: {'n_estimators': 363, 'learning_rate': 0.26513439823460105, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.7523723899895383, 'colsample_bytree': 0.7880706703120715, 'scale_pos_weight': 1.2318757749672438}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:54] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:57,024] Trial 8 finished with value: 0.597752808988764 and parameters: {'n_estimators': 343, 'learning_rate': 0.1682727599696017, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.8585280956540426, 'colsample_bytree': 0.8494304968033217, 'scale_pos_weight': 9.67262232855871}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:57,682] Trial 9 finished with value: 0.6022471910112359 and parameters: {'n_estimators': 234, 'learning_rate': 0.13755561296849725, 'max_depth': 11, 'min_child_weight': 9, 'subsample': 0.5142220505418218, 'colsample_bytree': 0.5300221425970674, 'scale_pos_weight': 2.4295257538321193}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:57] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:58,091] Trial 10 finished with value: 0.6112359550561798 and parameters: {'n_estimators': 119, 'learning_rate': 0.08534287909624844, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.5127684485203265, 'colsample_bytree': 0.9905635002980485, 'scale_pos_weight': 8.288400188488694}. Best is trial 0 with value: 0.6224719101123596.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:58] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:58,491] Trial 11 finished with value: 0.6247191011235955 and parameters: {'n_estimators': 198, 'learning_rate': 0.19689411117951314, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.6097067370243756, 'colsample_bytree': 0.6603930177444711, 'scale_pos_weight': 4.405419387496526}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:58] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:59,049] Trial 12 finished with value: 0.5955056179775281 and parameters: {'n_estimators': 186, 'learning_rate': 0.20787950376500675, 'max_depth': 6, 'min_child_weight': 6, 'subsample': 0.6111533844866843, 'colsample_bytree': 0.6364653584393573, 'scale_pos_weight': 4.311086185931158}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:44:59,411] Trial 13 finished with value: 0.6089887640449438 and parameters: {'n_estimators': 173, 'learning_rate': 0.08630617640673216, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.5976926880138334, 'colsample_bytree': 0.6498345091100248, 'scale_pos_weight': 3.6861292383501754}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:59] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:45:00,173] Trial 14 finished with value: 0.6 and parameters: {'n_estimators': 258, 'learning_rate': 0.19935269410428835, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.592339108944064, 'colsample_bytree': 0.9704906030355434, 'scale_pos_weight': 5.149573819646911}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:00] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:45:00,555] Trial 15 finished with value: 0.6202247191011236 and parameters: {'n_estimators': 154, 'learning_rate': 0.10151679996825717, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6447718095082534, 'colsample_bytree': 0.670703426738855, 'scale_pos_weight': 3.1591953604062173}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:00] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:45:01,034] Trial 16 finished with value: 0.5842696629213483 and parameters: {'n_estimators': 255, 'learning_rate': 0.207835206646002, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.5215522673718388, 'colsample_bytree': 0.6948295686698709, 'scale_pos_weight': 7.904428591948033}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:01] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:45:01,503] Trial 17 finished with value: 0.6179775280898876 and parameters: {'n_estimators': 149, 'learning_rate': 0.05833350695115044, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.5663071033692865, 'colsample_bytree': 0.5967446097663285, 'scale_pos_weight': 5.762767795979011}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:01] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:45:01,957] Trial 18 finished with value: 0.604494382022472 and parameters: {'n_estimators': 202, 'learning_rate': 0.01616130415576786, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.6806120852747348, 'colsample_bytree': 0.9090195206321111, 'scale_pos_weight': 8.603073429099483}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-25 07:45:03,002] Trial 19 finished with value: 0.5887640449438202 and parameters: {'n_estimators': 285, 'learning_rate': 0.14597371328113237, 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.5536140210410616, 'colsample_bytree': 0.8305233915494603, 'scale_pos_weight': 2.014613635073313}. Best is trial 11 with value: 0.6247191011235955.\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:03] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 198, 'learning_rate': 0.19689411117951314, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.6097067370243756, 'colsample_bytree': 0.6603930177444711, 'scale_pos_weight': 4.405419387496526}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 579\n",
      "[LightGBM] [Info] Number of data points in the train set: 3315, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:10] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:45:10] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 580\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 579\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 580\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 576\n",
      "[LightGBM] [Info] Number of data points in the train set: 2652, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.610192\n",
      "[LightGBM] [Info] Start training from score -1.608307\n",
      "Optimized Model Accuracy: 0.6292134831460674\n",
      "Optimized Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.76      0.66        76\n",
      "           1       0.21      0.19      0.20        21\n",
      "           2       0.58      0.52      0.55       139\n",
      "           3       0.69      0.75      0.72        48\n",
      "           4       0.73      0.68      0.71       161\n",
      "\n",
      "    accuracy                           0.63       445\n",
      "   macro avg       0.56      0.58      0.57       445\n",
      "weighted avg       0.63      0.63      0.63       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTENC, ADASYN\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Optuna for Hyperparameter Tuning\n",
    "import optuna\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "data = pd.read_csv('TA_cleaned.csv')  # 실제 파일 경로로 수정\n",
    "\n",
    "# 클래스 5와 9 제거 및 클래스 통합\n",
    "data_filtered = data[data['accTypeD'].isin([0, 1, 2, 3, 4, 6, 7, 8])]\n",
    "data_filtered['accTypeD_merged_combined'] = data_filtered['accTypeD'].replace({6: 2, 7: 2, 8: 2})\n",
    "\n",
    "# Feature Engineering\n",
    "data_filtered['grid_lat'] = (data_filtered['la_crd'] // 0.02) * 0.02\n",
    "data_filtered['grid_lon'] = (data_filtered['lo_crd'] // 0.02) * 0.02\n",
    "data_filtered['is_night'] = data_filtered['hour'].apply(lambda x: 1 if x < 6 or x >= 18 else 0)\n",
    "data_filtered['holiday_road_combo'] = data_filtered['is_holiday'].astype(str) + '_' + data_filtered['road_form_class'].astype(str)\n",
    "\n",
    "categorical_features = ['is_holiday', 'road_form_class', 'road_formD', 'carFLg',\n",
    "                        'carClassF', 'carClassVic', 'holiday_road_combo']\n",
    "\n",
    "numerical_features = ['hour', 'grid_lat', 'grid_lon', 'is_night']\n",
    "\n",
    "# 라벨 인코딩\n",
    "data_encoded = data_filtered.copy()\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data_encoded[col] = le.fit_transform(data_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X = data_encoded[numerical_features + categorical_features]\n",
    "y = data_encoded['accTypeD_merged_combined']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 증강 (ADASYN + SMOTENC)\n",
    "categorical_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "smotenc = SMOTENC(categorical_features=categorical_indices, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotenc.fit_resample(X_train, y_train)\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. 모델 정의 (Optuna 하이퍼파라미터 최적화)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1, 10),\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    preds = model.predict(X_test)\n",
    "    return accuracy_score(y_test, preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# 최적화된 XGBoost 모델\n",
    "xgb_model = XGBClassifier(**best_params)\n",
    "\n",
    "# StackingClassifier (메타 모델: CatBoost)\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),\n",
    "        ('catboost', CatBoostClassifier(random_state=42, verbose=0))\n",
    "    ],\n",
    "    final_estimator=CatBoostClassifier(random_state=42, verbose=0)\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "stacking_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 3. Threshold 최적화\n",
    "y_prob = stacking_model.predict_proba(X_test)\n",
    "threshold = 0.3  # Threshold 설정\n",
    "y_pred_adjusted = np.argmax(y_prob, axis=1)\n",
    "\n",
    "# 4. 성능 평가\n",
    "accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "report = classification_report(y_test, y_pred_adjusted, target_names=[str(cls) for cls in np.unique(y)])\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Optimized Model Accuracy:\", accuracy)\n",
    "print(\"Optimized Model Classification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
